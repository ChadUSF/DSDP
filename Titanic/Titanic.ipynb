{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bd6403",
   "metadata": {},
   "source": [
    "# Titanic - ML from disaster\n",
    "\n",
    "Use machine learning to create a model that predicts which passengers survived the Titanic shipwreck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee36d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training and test sets\n",
    "train_set = pd.read_csv(\"train.csv\")\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Review dataset\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns deemed arbitrary\n",
    "train = train_set.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "test = test_set.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66def556",
   "metadata": {},
   "source": [
    "# Split data (this was from original effort with one combined dataset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(titanic, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ba837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review shape of training data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review shape of test data\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target variable and remove target from input data.\n",
    "\n",
    "train_y = train[['Survived']]\n",
    "test_y = test[['Survived']]\n",
    "\n",
    "train_inputs = train.drop(['Survived'], axis=1)\n",
    "test_inputs = test.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c03fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reveiw training dataset\n",
    "train_inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review column types\n",
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b14a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Identify the categorical columns\n",
    "categorical_columns = train_inputs.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86888fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm numeric cols\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm cat cols\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f01702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cat pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)],\n",
    "        remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab26d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fit and transform the train data\n",
    "train_x = preprocessor.fit_transform(train_inputs)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06edf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a baseline accuracy using most frequent stra\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "dummy_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Train Accuracy\n",
    "dummy_train_pred = dummy_clf.predict(train_x)\n",
    "\n",
    "baseline_train_acc = accuracy_score(train_y, dummy_train_pred)\n",
    "\n",
    "print('Baseline Train Accuracy: {}' .format(baseline_train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Test Accuracy\n",
    "dummy_test_pred = dummy_clf.predict(test_x)\n",
    "\n",
    "baseline_test_acc = accuracy_score(test_y, dummy_test_pred)\n",
    "\n",
    "print('Baseline Test Accuracy: {}' .format(baseline_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single layer neural network model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "np.random.seed(13)\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "NN = keras.models.Sequential()\n",
    "\n",
    "NN.add(keras.layers.Input(shape=train_x.shape[1]))\n",
    "NN.add(keras.layers.Dense(16, activation='relu'))\n",
    "NN.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66354e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "NN.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb81589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = NN.fit(train_x, train_y, \n",
    "                    validation_data=(test_x, test_y), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0606ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = NN.evaluate(train_x, train_y, verbose=0)\n",
    "\n",
    "print(f\"Train {NN.metrics_names[0]}: {train_scores[0]:.2f}\")\n",
    "\n",
    "print(f\"Train {NN.metrics_names[1]}: {train_scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08280b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = NN.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "print(f\"Test {NN.metrics_names[0]}: {test_scores[0]:.2f}\")\n",
    "\n",
    "print(f\"Test {NN.metrics_names[1]}: {test_scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33846880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-layer Deep Neural Network model\n",
    "\n",
    "np.random.seed(13)\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "DNN = keras.models.Sequential()\n",
    "\n",
    "DNN.add(keras.layers.Input(shape=train_x.shape[1]))\n",
    "DNN.add(keras.layers.Dense(16, activation='relu'))\n",
    "DNN.add(keras.layers.Dense(12, activation='relu'))\n",
    "DNN.add(keras.layers.Dense(8, activation='relu'))\n",
    "DNN.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "DNN.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply early stopping to prevent overfitting\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(patience=3, verbose=1)\n",
    "\n",
    "callback = [earlystop]\n",
    "\n",
    "DNN.fit(train_x, train_y, validation_data=(test_x, test_y), \n",
    "            epochs=100, batch_size=100, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = DNN.evaluate(train_x, train_y, verbose=0)\n",
    "\n",
    "print(f\"Train {DNN.metrics_names[0]}: {train_scores[0]:.2f}\")\n",
    "\n",
    "print(f\"Train {DNN.metrics_names[1]}: {train_scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f18479",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = DNN.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "print(f\"Test {DNN.metrics_names[0]}: {test_scores[0]:.2f}\")\n",
    "\n",
    "print(f\"Test {DNN.metrics_names[1]}: {test_scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002674af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try a RandomForestClassifier model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, n_jobs=-1) \n",
    "\n",
    "rnd_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred = rnd_clf.predict(train_x)\n",
    "\n",
    "train_acc = accuracy_score(train_y, train_y_pred)\n",
    "\n",
    "print('Train acc: {}' .format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = rnd_clf.predict(test_x)\n",
    "\n",
    "test_acc = accuracy_score(test_y, test_y_pred)\n",
    "\n",
    "print('Test acc: {}' .format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576d66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
